{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this Notebook I'll be working on the **Amazon fine-food reviews** for ***sentiment analysis***.\n",
    "\n",
    "I will use a **RandomForestClassifier** and a **PassiveAggressiveClassifier**.\n",
    "The focus will be around the inconsistencies presented by the predictions of the model and how to overcome that issue.\n",
    "\n",
    "We will assess our model's **calibration level**, and after that we will try to calibrate it to the best of our abilities.\n",
    "\n",
    "Dear **Reader**, I hope this Notebook will spark curiosity in your mind and will eventually give you some information to start-off.\n",
    "\n",
    "Happy Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info about the Author\n",
    "\n",
    "***Youssef LAOUINA*** is a data science student at the **ExploreAI Academy**. He has a bachelor's degree in Economics and is fascinated by the data science world. He has a strong sence of the power in raw data and he is focused on building reliable Machine Learning models that cater for industy needs.\n",
    "\n",
    "> LinkedIn: https://www.linkedin.com/in/youssef-laouina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this code to time the execution\n",
    "# try:\n",
    "#     %load_ext autotime\n",
    "# except:\n",
    "#     !pip install ipython-autotime\n",
    "#     %load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data into pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outside Kaggle data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code bellow outside of Kaggle to download and load the dataset into a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import time\n",
    "#from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "#import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Kaggle API\n",
    "# api = KaggleApi()\n",
    "# api.authenticate()\n",
    "\n",
    "# # Define the dataset and the path where it will be saved\n",
    "# dataset = 'snap/amazon-fine-food-reviews'\n",
    "# path = './input' \n",
    "\n",
    "# # Create the directory if it does not exist\n",
    "# if not os.path.exists(path):\n",
    "#     os.makedirs(path)\n",
    "\n",
    "# def download_dataset(dataset, path):\n",
    "#     try:\n",
    "#         print(\"Downloading dataset:\", \"It will take a little while... Be patient!\", sep='\\n')\n",
    "#         api.dataset_download_files(dataset, path=path, unzip=True)\n",
    "#         print(\"Dataset loaded successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error downloading dataset: {e}\")\n",
    "        \n",
    "#         # Try to check internet connectivity\n",
    "#         try:\n",
    "#             response = requests.get(\"https://www.google.com\", timeout=5)\n",
    "#             response.raise_for_status()  # Raise an exception if there was a problem\n",
    "#             print(\"Internet connection seems to be working. The error might be related to the Kaggle server or dataset.\")\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             print(f\"Internet connection error: {e}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error checking internet connection: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_dataset(dataset, path)\n",
    "\n",
    "# # Load the data into a Pandas DataFrame\n",
    "# try:\n",
    "#     df = pd.read_csv(os.path.join(path, 'Reviews.csv'))\n",
    "#     print(\"Dataset loaded successfully into a Pandas DataFrame!\")\n",
    "# except FileNotFoundError:\n",
    "#     print(f\"Error: 'Reviews.csv' not found in the downloaded dataset.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading dataset into DataFrame: {e}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inside Kaggle data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset from the working directory into a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/kaggle/input/amazon-fine-food-reviews/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Amazon Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on the data types and counts of each feature\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_products_unique = df.ProductId.nunique()\n",
    "print(f\"We have {count_products_unique: ,} unique products.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each product's number of reviews: \n",
    "count_reviews_per_product = df.groupby(by='ProductId').UserId.count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "gs = gridspec.GridSpec(2, 2, width_ratios=[2, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:, 0])\n",
    "n = len(count_reviews_per_product[:500])\n",
    "sns.lineplot(y=count_reviews_per_product[:500], x=list(range(n)), ax=ax1)\n",
    "ax1.set_title('Distribution of Reviews per Product - Top 500')\n",
    "ax1.set_xlabel('Product ID - genric')\n",
    "ax1.set_ylabel('Number of Reviews')\n",
    "plt.annotate(\n",
    "    '913 Reviews',  # Text\n",
    "    xy=(0, 913),  # Point to annotate\n",
    "    xytext=(100, 800),  # Text location\n",
    "    arrowprops=dict(facecolor='red')  # Arrow properties\n",
    ")\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "sns.barplot(y=count_reviews_per_product[:6], x=count_reviews_per_product[:6].index, ax=ax2)\n",
    "plt.xticks(rotation=20)\n",
    "ax2.set_title('Top 5 Products - Max Reviews')\n",
    "ax2.set_xlabel('Product ID')\n",
    "ax2.set_ylabel('Number of Reviews')\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "palette = ['blue', 'orange', 'green', 'red', 'purple', 'brown']\n",
    "sns.barplot(y=count_reviews_per_product[-7:-1], x=count_reviews_per_product[-7:-1].index, ax=ax3, palette=palette)\n",
    "plt.xticks(rotation=-20)\n",
    "ax3.set_title('Top 5 Products - Min Reviews')\n",
    "ax3.set_xlabel('Product ID')\n",
    "ax3.set_ylabel('Number of Reviews')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**:\n",
    "\n",
    "We can see clearly that very few products have lots of reviews and the discrepancies between reviews for the top reviews both min and max are widely seperate. This is helpful for us to be aware when buliding and training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentages of each star review\n",
    "pct_reviews = df.Score.value_counts() * 100 / df.Score.value_counts().sum()\n",
    "\n",
    "# merge the percentages and the corresponding star into a dictionary\n",
    "pct_dict = dict(zip(pct_reviews.keys(), pct_reviews.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of reviews made by each User\n",
    "userid_count = df.groupby(by='UserId').UserId.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.histplot(userid_count.unique(), kde=True, ax=axes[0])\n",
    "axes[0].set_title('Distribution of Unique # of Reviews per User - Amazon')\n",
    "axes[0].set_xlabel('# of Reviews')\n",
    "axes[0].set_ylabel('Count of Users')\n",
    "axes[0].grid(axis='x') # remove vertical grid lines\n",
    "\n",
    "axes[1].pie(pct_reviews, labels=pct_dict, startangle=180, autopct='%1.0f%%')\n",
    "axes[1].set_title('Star Reviews Distribution - Amazon')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**:\n",
    "\n",
    "It appears that users who write reviews a lot are very few in comparaison to those who write few reviews. This could be due to product popularity among consumers. In addition, the distribution of star reviews is highly biased towards 5-stars. We should be careful with this data because it can transform bias to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform ratings' range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 being negative review\n",
    "# 1 being neutral reviews\n",
    "# 2 being positive reviews \n",
    "\n",
    "def update_score(score):\n",
    "    if score <= 2:\n",
    "        return 0\n",
    "    elif score == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_score'] = df.Score.apply(update_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-sampling: Balance Reviews to minimize Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_counts = df.new_score.value_counts()\n",
    "\n",
    "df.new_score.value_counts()\n",
    "sns.barplot(x=df_val_counts.index, y=df_val_counts)\n",
    "plt.xlabel('Scores')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = pd.DataFrame(columns=df.columns)\n",
    "grouped = df.groupby(by='new_score')\n",
    "\n",
    "# set the minimum number of reviews across all categories\n",
    "min_count = df.new_score.value_counts()[1] # minimum count of all scores is score 1\n",
    "\n",
    "# Sample the minimum number of reviews from each category and append to the balanced DataFrame\n",
    "for score, group in grouped:\n",
    "    sampled_group = group.sample(n=min_count, random_state=42)\n",
    "    balanced_df = pd.concat([balanced_df, sampled_group], ignore_index=True)\n",
    "\n",
    "# Shuffle the balanced DataFrame (optional)\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can verify that our reviews now are balanced in terms of size\n",
    "df_val_counts_balanced = balanced_df.groupby('new_score').Id.count()\n",
    "\n",
    "sns.barplot(x=df_val_counts_balanced.index, y=df_val_counts_balanced)\n",
    "plt.xlabel('Scores')\n",
    "plt.title('Distribution of Scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews = df.shape[0]\n",
    "\n",
    "print(f\"We have a total of {total_reviews: ,} reviews in our dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reviews_balanced = balanced_df.shape[0]\n",
    "\n",
    "short_df = balanced_df\n",
    "print(f\"We have a total of {total_reviews_balanced: ,} reviews in our balanced dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
    "            'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' '] # space here acts like a natural separator\n",
    "\n",
    "# Filter corpus to only inlcude english alphabets and replace non-word characters to space including punctuations\n",
    "def filter_corpus(document):\n",
    "    filtered_doc = ''.join([char if char in alphabet else ' ' for char in document.lower()])\n",
    "    return ' '.join(filtered_doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.Text = short_df.Text.apply(filter_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata\n",
    "\n",
    "all_punctuation = {c for c in string.punctuation}\n",
    "all_punctuation |= {c for c in map(chr, range(0x2000, 0x206F + 1)) if unicodedata.category(c).startswith('P')}\n",
    "all_punctuation |= {c for c in map(chr, range(0x2070, 0x209F + 1)) if unicodedata.category(c).startswith('P')}\n",
    "all_punctuation |= {c for c in map(chr, range(0x3000, 0x303F + 1)) if unicodedata.category(c).startswith('P')}\n",
    "\n",
    "# remove all punctuations that are not utf-8 \n",
    "def rmv_puncs(document):\n",
    "    cleaned_doc = ''.join([char for char in document if char not in all_punctuation])\n",
    "    return ' '.join(cleaned_doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rmv_puncs function\n",
    "short_df.Text = short_df.Text.apply(rmv_puncs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "# tokenize the text\n",
    "def tokenizer(text):\n",
    "    return TreebankWordTokenizer().tokenize(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply tokenize function\n",
    "short_df['tokens'] = short_df.Text.apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop words: custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not remove all stop words from the corpus because sometimes those stop words carry a piece of information that our model can benefit from.\n",
    "\n",
    "For example if a user says: 'Not Bad' this automatically mean 'Good' and we need  our model to learn from situations like these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# standard stop words list\n",
    "standard_stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# List of important stop words to retain in our case\n",
    "important_stop_words = {'not', 'no', 'but', 'against', 'don', \"don't\", 'won', \"won't\", 'isn', \"isn't\", 'aren', \"aren't\", 'couldn',\n",
    "                        \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\",\n",
    "                        'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn',\n",
    "                        \"wasn't\", 'weren', \"weren't\", 'wouldn', \"wouldn't\"}\n",
    "\n",
    "# Custom stop words list\n",
    "custom_stop_words = standard_stop_words - important_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply rmv_stopwords function\n",
    "def rmv_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in custom_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply rmv_stopwords function\n",
    "short_df.tokens = short_df.tokens.apply(rmv_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove additional noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all tokens that has  less than 2 characters\n",
    "def rmv_noise(tokens):\n",
    "    return [token for token in tokens if len(token) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.tokens = short_df.tokens.apply(rmv_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced stemming: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('wordnet') \n",
    "#!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n",
    "\n",
    "def lemmatizer(tokens):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(token) for token in tokens]\n",
    "\n",
    "# to test the object:\n",
    "#wnl = WordNetLemmatizer()\n",
    "#print(wnl.lemmatize('dogs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df['lemmas'] = short_df.tokens.apply(lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct text after cleaning it\n",
    "def reconstruct_text(lemmas):\n",
    "    return ' '.join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_df.Text = short_df.lemmas.apply(reconstruct_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will subdivide our dataset to training set, testing set and we will keep a calibration set for our calibration.\n",
    "train_size = 0.6\n",
    "calib_size = 0.05\n",
    "test_size = 1 - train_size - calib_size  # 0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = short_df.Text\n",
    "y = short_df.new_score.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with calibration dataset and Testing Dataset\n",
    "X_train_calib, X_test, y_train_calib, y_test = train_test_split(X,\n",
    "                                                                y,\n",
    "                                                                test_size=test_size,\n",
    "                                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset and Calibration Dataset\n",
    "X_train, X_calib, y_train, y_calib = train_test_split(X_train_calib,\n",
    "                                                      y_train_calib,\n",
    "                                                      test_size=calib_size/(1 - test_size), \n",
    "                                                      random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_shape = [X_train.shape[0], X_test.shape[0], X_calib.shape[0]]\n",
    "data_name = ['Train', 'Test', 'Calibration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=data_name, y=data_shape)\n",
    "plt.title('Sizes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining CountVectorizer and OneVsRestClassifier\n",
    "# change the number of estimators according to your machine power (ps: 1,000 is more than enough.)\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=10, n_jobs=-1, random_state=42, criterion='log_loss'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilistic predictions (voting percentages)\n",
    "test_probs = model.predict_proba(X_test)\n",
    "\n",
    "# get hard predictions \n",
    "hard_preds = model.predict(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, hard_preds)}')\n",
    "print(f'Log-Loss: {log_loss(y_test, test_probs)}')\n",
    "\n",
    "print(classification_report(y_test, hard_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Calibration: Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ml_insights as mli \n",
    "except:\n",
    "    !pip install ml_insights\n",
    "    import ml_insights as mli "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for each class\n",
    "# we use the test_probs and not the hard predictions\n",
    "for i in range(test_probs.shape[1]):\n",
    "    fig = plt.figure(figsize=(10, 5)) \n",
    "    mli.plot_reliability_diagram(y_test == i, test_probs[:, i], show_histogram=True)\n",
    "    plt.title(f'Probability Distribution of class {i}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Reliability Diagram** of our RandomForestClassifier seems to be not well calibrated.\n",
    "\n",
    "We can observe that some data points are outside the CI and there is a consistent over predicting in small values and over predicting in larger values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Platt Scaling\n",
    "Assumes that there is a logistic relationship between the scores $z$ and the true probability $p$.\n",
    "\n",
    "$\\log\\left(\\frac{p}{1-p}\\right) = \\alpha + \\beta z$\n",
    "\n",
    "$p = \\frac{1}{1+\\exp(-(\\alpha + \\beta z))}$\n",
    "\n",
    "So it fits the two parameters $\\alpha$ and $\\beta$ just like in logistic regression!\n",
    "\n",
    "- Very restrictive set of possible functions\n",
    "- Needs very little data\n",
    "- Historically, came from the observation (and subsequent theoretical arguments) that a logistic regression was the \"right\" calibration for Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: Platt, J. (1999). Probabilistic outputs for support vector machines and comparison to regularized likelihood methods. Advances in Large Margin Classifiers (pp.61â€“74)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit Platt scaling (logistic calibration)\n",
    "# the big C nubmber is to unregularize the model (simply to turn-off the regularization)\n",
    "lr = LogisticRegression(C=99_999_999_999, solver='lbfgs', n_jobs=-1)\n",
    "lr.fit(test_probs, y_test)\n",
    "\n",
    "# get the calibration-set probabilistic predictions\n",
    "calib_probs = model.predict_proba(X_calib)\n",
    "\n",
    "# calibrated calibration-set probabilistic predictions\n",
    "calib_preds_calibrated = lr.predict_proba(calib_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "hard_preds_calib = model.predict(X_calib)\n",
    "print(f'Accuracy: {accuracy_score(y_calib, hard_preds_calib)}')\n",
    "print(f'Log-Loss: {log_loss(y_calib, calib_preds_calibrated)}')\n",
    "\n",
    "print(classification_report(y_calib, hard_preds_calib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teh **Log-Loss** has decreased in comparaison to the the one measured for the uncalibrated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for each class\n",
    "for i in range(calib_probs.shape[1]):\n",
    "    fig = plt.figure(figsize=(10, 5)) \n",
    "    mli.plot_reliability_diagram(y_calib == i, calib_preds_calibrated[:, i], show_histogram=True)\n",
    "    plt.title(f'Probability Distribution of class {i}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that after calibration, we tend to fall in the acceptable region where we do not over predict nor under predict.\n",
    "\n",
    "We also notice some jumping behaviors of certain data points but it is mainly due to small bin values which is verified by the **Probability distribution diagram** in the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"This coffee has a rich and robust flavor that I absolutely love. It's my go-to every morning, and I can't imagine starting my day without it. The aroma is invigorating, and the taste is smooth without any bitterness. I appreciate the sustainable sourcing, which makes me feel good about my purchase. Definitely a staple in my pantry.\",\n",
    "    \"The pasta was okay, but I didn't find it to be anything special. It cooked well and had an acceptable texture, but the flavor was somewhat bland. I've had better from other brands at a similar price point. It's not terrible, but I wouldn't go out of my way to buy it again. It serves its purpose but doesn't stand out.\",\n",
    "    \"I was very disappointed with this tea. It tasted stale and did not have the aroma I expected. I usually enjoy a strong, fragrant cup of tea, but this one fell flat. Even after steeping it longer than usual, the flavor was weak and uninspiring. I will not be purchasing this brand again, as it does not live up to its description.\",\n",
    "    \"These cookies are fantastic! They have the perfect amount of sweetness and a great texture. Each bite is delightful, with a nice balance between the crunchiness of the cookie and the softness of the filling. I also appreciate the high-quality ingredients used. These cookies have quickly become a favorite treat in my household.\",\n",
    "    \"The olive oil is good, but I've had better. It's a decent option for the price, with a pleasant enough flavor for everyday cooking. However, it lacks the depth and richness that I've experienced with some premium brands. It's a solid choice if you're on a budget, but don't expect it to elevate your dishes significantly.\",\n",
    "    \"The chocolate was too bitter for my taste, and I didn't enjoy it at all. I generally like dark chocolate, but this one was overwhelmingly bitter and left an unpleasant aftertaste. The texture was also a bit chalky. I tried to use it in baking to see if it would be better, but it still didn't meet my expectations.\",\n",
    "    \"I love this granola! It's crunchy, flavorful, and perfect for breakfast or a snack. The mix of nuts and dried fruits adds a wonderful variety of textures and flavors. It's not overly sweet, which I appreciate, and it's great with yogurt or milk. I've even started carrying a small bag with me for a quick, healthy snack on the go.\",\n",
    "    \"The sauce was mediocre. It lacked the depth of flavor I was hoping for and was somewhat watery. It did the job for a quick weeknight meal, but I found myself needing to add additional spices and ingredients to make it more palatable. It's not bad, but there are definitely better options available.\",\n",
    "    \"This honey is amazing! It's smooth, rich, and has a wonderful floral note that adds a unique touch to everything I use it in. Whether it's in tea, on toast, or as part of a recipe, this honey never disappoints. The quality is evident from the first taste, and I love that it's sourced from local, organic farms.\",\n",
    "    \"The chips were stale and didn't taste fresh. I wouldn't buy them again. When I opened the bag, I was immediately struck by the lack of crispness. The flavor was also off, almost as if they had been sitting on the shelf for too long. This was a disappointing purchase, and I had to throw most of the bag away.\"\n",
    "]\n",
    "\n",
    "# True scores\n",
    "scores = [2, 1, 0, 2, 1, 0, 2, 1, 2, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilities comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncalibrated Vs. Calibrated Probabilities for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_df = pd.DataFrame(reviews, columns=['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(doc):\n",
    "    document = filter_corpus(doc)\n",
    "    document = rmv_puncs(document)\n",
    "    tokens = tokenizer(document)\n",
    "    tokens = rmv_stopwords(tokens)\n",
    "    tokens = rmv_noise(tokens)\n",
    "    lemmas = lemmatizer(tokens)\n",
    "    text = reconstruct_text(lemmas)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews = rev_df.reviews.apply(clean_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_prob = model.predict_proba(cleaned_reviews)\n",
    "print('Uncalibrated Prob:')\n",
    "pd.DataFrame(uncal_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_prob = lr.predict_proba(uncal_prob)\n",
    "print('Calibrated Prob:')\n",
    "pd.DataFrame(cal_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_preds_sample_uncalib = model.predict(cleaned_reviews)\n",
    "hard_preds_sample_calib = lr.predict(uncal_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Uncalibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_uncalib)}')\n",
    "print(f'Log-Loss: {log_loss(scores, uncal_prob)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Calibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_calib)}')\n",
    "print(f'Log-Loss: {log_loss(scores, cal_prob)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SplineCalib \n",
    "- SplineCalib fits a cubic smoothing spline to the relationship between the uncalibrated scores and the calibrated probabilities\n",
    "- Smoothing splines strike a balance between fitting the points well and having a smooth function\n",
    "- SplineCalib uses a smoothed logistic function - so the fit to data is measured by likelihood (i.e. log-loss) and the smoothness refers to the integrated second derivative **before** the logistic transformation.\n",
    "- There is a nuisance parameter that trades off smoothness for fit.  At one extreme it will revert to standard logistic regression (i.e. Platt scaling) and at the other extreme it will be a very wiggly function that fits the data but does not generalize well.\n",
    "\n",
    "- SplineCalib automatically fits the nuisance parameter (though this can be adjusted by the user)\n",
    "- The resulting calibration function is not necessarily monotonic.  (In some cases this may be beneficial)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: Lucena, B. Spline-based Probability Calibration. https://arxiv.org/abs/1809.07751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_mc = mli.SplineCalib(unity_prior=False)\n",
    "calib_mc.fit(test_probs, y_test, verbose=True)\n",
    "\n",
    "# calibrated calibration-set probabilistic predictions \n",
    "calib_preds_calibrated_spl = calib_mc.calibrate(calib_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "hard_preds_calib_spl = model.predict(X_calib)\n",
    "print(f'Accuracy: {accuracy_score(y_calib, hard_preds_calib_spl)}')\n",
    "print(f'Log-Loss: {log_loss(y_calib, calib_preds_calibrated_spl)}')\n",
    "\n",
    "print(classification_report(y_calib, hard_preds_calib_spl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for each class\n",
    "for i in range(calib_probs.shape[1]):\n",
    "    fig = plt.figure(figsize=(10, 5)) \n",
    "    mli.plot_reliability_diagram(y_calib == i, calib_preds_calibrated_spl[:, i], show_histogram=True)\n",
    "    plt.title(f'Probability Distribution of class {i}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilities comparaison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncalibrated Vs. Calibrated Probabilities for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_prob = model.predict_proba(cleaned_reviews)\n",
    "print('Uncalibrated Prob:')\n",
    "pd.DataFrame(uncal_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_prob_spl = calib_mc.predict_proba(uncal_prob)\n",
    "print('Calibrated Prob:')\n",
    "pd.DataFrame(cal_prob_spl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_preds_sample_uncalib = model.predict(reviews) #np.argmax(uncal_prob, axis=1)\n",
    "hard_preds_sample_calib_spl = np.argmax(cal_prob_spl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Uncalibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_uncalib)}')\n",
    "print(f'Log-Loss: {log_loss(scores, uncal_prob)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Calibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_calib_spl)}')\n",
    "print(f'Log-Loss: {log_loss(scores, cal_prob_spl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = cleaned_reviews\n",
    "y_t = pd.Series(scores).astype(int)\n",
    "\n",
    "# Predicting\n",
    "predictions = model.predict(X_t)\n",
    "predictions_proba = model.predict_proba(X_t)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_t, y_pred=predictions)}')\n",
    "print(f'Log-Loss: {log_loss(y_true=y_t, y_pred=predictions_proba)}')\n",
    "\n",
    "print(classification_report(y_true=y_t, y_pred=predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building: PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pipeline combining CountVectorizer and OneVsRestClassifier\n",
    "PA_pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 3))),\n",
    "    ('classifier', OneVsRestClassifier(\n",
    "        PassiveAggressiveClassifier(C=0.1, shuffle=False, fit_intercept=False, validation_fraction=0.2, tol=1e-4))\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best estimator found by Grid Search\n",
    "PA_model = PA_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "y_pred_pa = PA_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred_pa)}')\n",
    "print(classification_report(y_test, y_pred_pa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing Calibration: Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform predictions gotten from PassiveAggressiveClassifier into Probabilistic Predictions using a SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw decision function scores\n",
    "decision_function_scores_test = PA_pipeline.decision_function(X_test)\n",
    "\n",
    "# we'll use a model to train it on decision function scores to get probabilistic predections\n",
    "pp_model = SVC(probability=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted \"probabilities\" (membership probabilities)\n",
    "pp_model_test_probs = np.zeros_like(decision_function_scores_test)\n",
    "\n",
    "for i in range(decision_function_scores_test.shape[1]):\n",
    "    pp_model.fit(decision_function_scores_test[:, [i]], y_test == i)\n",
    "    pp_model_test_probs[:, i] = pp_model.predict_proba(decision_function_scores_test[:, [i]])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Reliability diagrams for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for each class\n",
    "# we use the test_probs and not the hard predictions\n",
    "for i in range(decision_function_scores_test.shape[1]):\n",
    "    fig = plt.figure(figsize=(10, 5)) \n",
    "    mli.plot_reliability_diagram(y_test == i, pp_model_test_probs[:, i], show_histogram=True)\n",
    "    plt.title(f'Probability Distribution of class {i}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Platt Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit Platt scaling (logistic calibration)\n",
    "# the big C nubmber is to unregularize the model (simply to turn-off the regularization)\n",
    "lr = LogisticRegression(C=99_999_999_999, solver='lbfgs', n_jobs=-1)\n",
    "lr.fit(pp_model_test_probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the calibration-set predictions from PassiveAggressiveClassifier\n",
    "calib_preds = PA_pipeline.decision_function(X_calib)\n",
    "\n",
    "# Get the predicted \"probabilities\" (membership probabilities)\n",
    "pp_model_calib_probs = np.zeros_like(calib_preds)\n",
    "\n",
    "for i in range(calib_preds.shape[1]):\n",
    "    pp_model.fit(calib_preds[:, [i]], y_calib == i)\n",
    "    pp_model_calib_probs[:, i] = pp_model.predict_proba(calib_preds[:, [i]])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrated calibration-set probabilistic predictions\n",
    "calib_preds_calibrated = lr.predict_proba(pp_model_calib_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "hard_preds_calib = PA_pipeline.predict(X_calib)\n",
    "print(f'Accuracy: {accuracy_score(y_calib, hard_preds_calib)}')\n",
    "print(f'Log-Loss: {log_loss(y_calib, calib_preds_calibrated)}')\n",
    "\n",
    "print(classification_report(y_calib, hard_preds_calib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for each class\n",
    "for i in range(pp_model_calib_probs.shape[1]):\n",
    "    fig = plt.figure(figsize=(10, 5)) \n",
    "    mli.plot_reliability_diagram(y_calib == i, calib_preds_calibrated[:, i], show_histogram=True)\n",
    "    plt.title(f'Probability Distribution of class {i}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilities comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_pred = PA_pipeline.decision_function(cleaned_reviews)\n",
    "\n",
    "# Get the predicted \"probabilities\" (membership probabilities)\n",
    "uncal_prob = np.zeros_like(uncal_pred)\n",
    "\n",
    "for i in range(uncal_pred.shape[1]):\n",
    "    pp_model.fit(uncal_pred[:, [i]], np.array(scores) == i)\n",
    "    uncal_prob[:, i] = pp_model.predict_proba(uncal_pred[:, [i]])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uncalibrated Prob:')\n",
    "pd.DataFrame(uncal_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_prob = lr.predict_proba(uncal_prob)\n",
    "print('Calibrated Prob:')\n",
    "pd.DataFrame(cal_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_preds_sample_uncalib = PA_pipeline.predict(reviews)\n",
    "hard_preds_sample_calib = np.argmax(cal_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Uncalibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_uncalib)}')\n",
    "print(f'Log-Loss: {log_loss(scores, uncal_prob)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Calibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_calib)}')\n",
    "print(f'Log-Loss: {log_loss(scores, cal_prob)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: SplineCalib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_mc = mli.SplineCalib(unity_prior=False)\n",
    "calib_mc.fit(pp_model_test_probs, y_test, verbose=True)\n",
    "\n",
    "# calibrated calibration-set probabilistic predictions \n",
    "calib_preds_calibrated_spl = calib_mc.calibrate(pp_model_calib_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "hard_preds_calib_spl = PA_pipeline.predict(X_calib) #np.argmax(calib_preds_calibrated_spl, axis=1)\n",
    "print(f'Accuracy: {accuracy_score(y_calib, hard_preds_calib_spl)}')\n",
    "print(f'Log-Loss: {log_loss(y_calib, calib_preds_calibrated_spl)}')\n",
    "\n",
    "print(classification_report(y_calib, hard_preds_calib_spl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Reliability diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reliability diagrams for each class\n",
    "for i in range(calib_preds_calibrated_spl.shape[1]):\n",
    "    fig = plt.figure(figsize=(10, 5)) \n",
    "    mli.plot_reliability_diagram(y_calib == i, calib_preds_calibrated_spl[:, i], show_histogram=True)\n",
    "    plt.title(f'Probability Distribution of class {i}')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibrated model: Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilities comparaison "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncalibrated Vs. Calibrated Probabilities for our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uncalibrated Prob:')\n",
    "pd.DataFrame(uncal_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_prob_spl = calib_mc.predict_proba(uncal_prob)\n",
    "print('Calibrated Prob:')\n",
    "pd.DataFrame(cal_prob_spl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_preds_sample_uncalib = np.argmax(uncal_prob, axis=1)\n",
    "hard_preds_sample_calib_spl = np.argmax(cal_prob_spl, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Uncalibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_uncalib)}')\n",
    "print(f'Log-Loss: {log_loss(scores, uncal_prob)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Calibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores, hard_preds_sample_calib_spl)}')\n",
    "print(f'Log-Loss: {log_loss(scores, cal_prob_spl)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our model on testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = cleaned_reviews\n",
    "y_t = pd.Series(scores).astype(int)\n",
    "\n",
    "# Predicting\n",
    "predictions = PA_pipeline.predict(X_t)\n",
    "\n",
    "# Get the raw decision function scores\n",
    "X_t_scores = PA_pipeline.decision_function(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted \"probabilities\" (membership probabilities)\n",
    "pp_model_probs = np.zeros_like(X_t_scores)\n",
    "\n",
    "for i in range(X_t_scores.shape[1]):\n",
    "    pp_model.fit(X_t_scores[:, [i]], y_t == i)\n",
    "    pp_model_probs[:, i] = pp_model.predict_proba(X_t_scores[:, [i]])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_proba = pp_model_probs\n",
    "\n",
    "# Evaluate the model\n",
    "print(f'Accuracy: {accuracy_score(y_true=y_t, y_pred=predictions)}')\n",
    "print(f'Log-Loss: {log_loss(y_true=y_t, y_pred=predictions_proba)}')\n",
    "\n",
    "print(classification_report(y_true=y_t, y_pred=predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model reusability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_path = './model/'\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "# Save the pipeline to disk for reusability\n",
    "pipeline_model = os.path.join(model_path, \"PA_pipeline.pkl\")\n",
    "joblib.dump(PA_pipeline, pipeline_model)\n",
    "\n",
    "# Save the probabilities predictions model to disk for reusability\n",
    "proba_pred_model = os.path.join(model_path, \"Proba_pred_model.pkl\")\n",
    "joblib.dump(pp_model, proba_pred_model)\n",
    "\n",
    "# Save the calibrated model to disk for reusability\n",
    "platt_calibration_model = os.path.join(model_path, \"Platt_calibration_model.pkl\")\n",
    "joblib.dump(lr, platt_calibration_model);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncalib_model = joblib.load(pipeline_model)\n",
    "proba_model = joblib.load(proba_pred_model)\n",
    "calib_model = joblib.load(platt_calibration_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model reusability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_2 = [\n",
    "    \"The pasta was overcooked and tasteless, making the entire dish unenjoyable. The sauce lacked any depth of flavor and felt more like a watery mix of tomatoes. I was very disappointed and won't be ordering from here again.\",\n",
    "    \"The service was decent, but the food was nothing special. The appetizers were bland and the main course lacked the seasoning to make it memorable. However, the dessert was the saving grace, with a rich and creamy texture that was quite satisfying.\",\n",
    "    \"Amazing flavors and perfect seasoning! Every bite of the roasted chicken was a delightful experience. The sides were equally impressive, especially the garlic mashed potatoes. This was by far the best meal I've had in a long time, and I will definitely be returning soon.\",\n",
    "    \"The dessert was okay, but the main course was a letdown. The steak was overcooked and the vegetables were soggy. It was not the fine dining experience I was expecting, although the ambiance was pleasant and the staff were attentive.\",\n",
    "    \"Great ambiance and friendly staff made the evening enjoyable. The steak was cooked to perfection, juicy and tender, and the accompanying wine selection was excellent. It was a fantastic dining experience that I would highly recommend.\",\n",
    "    \"The soup was too salty and the bread was stale, making it hard to enjoy the starter. The main course didn't fare much better, with the chicken being dry and over-seasoned. Overall, it was a disappointing meal that I wouldn't recommend.\",\n",
    "    \"I had an average experience at this restaurant. The atmosphere was nice and the staff were polite, but the food was just mediocre. The pasta was underwhelming, and while the dessert was tasty, it wasn't enough to make up for the rest of the meal.\",\n",
    "    \"Excellent presentation and delicious taste! The seafood platter was fresh and flavorful, with a great variety of options. The service was prompt and courteous, adding to a wonderful dining experience. I highly recommend this place for seafood lovers.\",\n",
    "    \"The salad was fresh and the dressing was delightful, providing a great start to the meal. The main course, a perfectly cooked salmon, was equally impressive. The flavors were well-balanced and the portions were generous. This is a place I would visit again.\",\n",
    "    \"Not impressed with the quality of the ingredients used. The vegetables in the stir-fry were limp and lacked flavor, and the sauce was too sweet for my taste. The overall experience felt like a fast-food joint rather than a fine dining restaurant.\"\n",
    "]\n",
    "\n",
    "scores_2 = [0, 1, 2, 1, 2, 0, 1, 2, 2, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_2 = pd.DataFrame(reviews_2, columns=['reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_reviews_2 = reviews_2.reviews.apply(clean_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_pred_2 = uncalib_model.decision_function(cleaned_reviews_2)\n",
    "\n",
    "# Get the predicted \"probabilities\" (membership probabilities)\n",
    "uncal_prob_2 = np.zeros_like(uncal_pred_2)\n",
    "\n",
    "for i in range(uncal_pred_2.shape[1]):\n",
    "    proba_model.fit(uncal_pred_2[:, [i]], np.array(scores_2) == i)\n",
    "    uncal_prob_2[:, i] = proba_model.predict_proba(uncal_pred_2[:, [i]])[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uncalibrated Prob:')\n",
    "pd.DataFrame(uncal_prob_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_prob_2 = calib_model.predict_proba(uncal_prob_2)\n",
    "print('Calibrated Prob:')\n",
    "pd.DataFrame(cal_prob_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_preds_sample_uncalib_2 = uncalib_model.predict(cleaned_reviews_2)\n",
    "hard_preds_sample_calib_2 = np.argmax(cal_prob_2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Uncalibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores_2, hard_preds_sample_uncalib_2)}')\n",
    "print(f'Log-Loss: {log_loss(scores_2, uncal_prob_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print('Calibrated model:')\n",
    "print(f'Accuracy: {accuracy_score(scores_2, hard_preds_sample_calib_2)}')\n",
    "print(f'Log-Loss: {log_loss(scores_2, cal_prob_2)}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 18,
     "sourceId": 2157,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
